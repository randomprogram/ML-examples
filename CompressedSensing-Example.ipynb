{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/randomprogram/ML-examples/blob/master/CompressedSensing-Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kcA9OZKbiowO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Compressed Sensing example.**\n",
        "##General Theory\n",
        "\n",
        "Consider the following linear equation:\n",
        "\n",
        "$A\\cdot X =b$, where $A\\in R^{m\\times n}$, $X \\in R^{n \\times 1}$ and $b \\in R^{n \\times 1}$. \n",
        "\n",
        "when $m < n$ the system is underdetermined and there are infinite number of $X$ that satisfy the solution. However, there is recent theoratical and practical efforts that show that when $X$ is sparse (meaning many of the elements of $X$ are zero ) and matrix $A$ satistifies certain requirement, $X$ can be recovered exactly. Theory proves that when the entries of $A$ are random, $X$ can be recovered with probability approaching 1. In practice, $A$ can be random selection of Fourier transformation or discrete consine transformation. Under the condition that the measurement of $b$ is noisy, which is often the case, the solution is obtained by solving the following convex optimization problem:\n",
        "\n",
        "$X^* = \\underset{X}{argmin} { } |A\\cdot X - b|_2^2 + \\lambda |X|_1$\n",
        "\n",
        "where the first term on the right hand side of the equation encourages data fit, and the second term is the L1 norm of $X$, which is what drives the solution to be sparse compared to say a L2 norm.\n",
        "\n",
        "References: \n",
        "\n",
        "##Demo\n",
        "Codes are partically based on [this nice demo](http://www.pyrunner.com/weblog/2016/05/26/compressed-sensing-python/)"
      ]
    }
  ]
}